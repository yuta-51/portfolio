---
layout: post
title:  "MLB Salary Data ETL"
info:  "Loading webscraped, processed MLB data into a Postgre Database."
tech: "Python, PostgreSQL"
type: Project
thumbnail: https://upload.wikimedia.org/wikipedia/en/thumb/a/a6/Major_League_Baseball_logo.svg/1200px-Major_League_Baseball_logo.svg.png
---

# Goal
The goal for this project is to create an ETL process that will ultimately load player salary data into a PostgreSQL data base. 




# Tools
Python
- pandas
- requests
- beautifulsoup
- psycopg2
- re (regex)  
PostgreSQL
- Locally hosted database


# Procedure

# Step 1: Scrape Salary Data 
The website we will scrape data from is located [here](https://legacy.baseballprospectus.com/compensation/). The main table contains the total payroll for each MLB team. To reach the salary of individual players, the hyperlink within the table must be accessed. The hyperlink urls from the main table are scraped first so each team's player salary table can be accessed in a systematic way. To do this, the requests and beautifulsoup libraries are used to parse the website's html. A function is written to do these taks.


```python
def get_soup(url):
    r = requests.get(url)
    return BeautifulSoup(r.text, 'html.parser')
```

By searching for the ```table``` element in the html page with the specific class the website assigns it, we can identtify the table that we want to scrape data from. 


```python
def get_table_as_df(soup):
    table = soup.find_all("table", class_="display striped enhancedtable")
    scraped_table = [[0]]
    cols = get_header_as_list(table[0])
    for row in table[0].find_all('tr'):
        row_data = []
        for cell in row.find_all('td'):
            row_data.append(cell.text.strip().replace('\n', ''))
        scraped_table.append(row_data)

    # Dropped the row after the header row because it is always full on Null values
    return pd.DataFrame(scraped_table, columns= cols).drop([0, 1]).reset_index(drop=True)
```

This function ```get_table_as_df``` scrapes data into a 2-d list then creates a dataframe out of it. 


```python
def get_header_as_list(table):
    header = []
    for th in table.find_all("th"):
        header.append(th.text.replace(' ', '_').replace('/', '_per_').replace('$M', 'usd_million').replace('$', 'usd').lower())
    return header
```

The ```get_header_as_list``` function is a helper function which scrapes just the column names off of the table so that we can have a header in the dataframe. 


Using these two functions together, we can obtain a dataframe containing the table data on a specific website. 

```python
base_url = "https://legacy.baseballprospectus.com/compensation/"
soup = get_soup(base_url)
table = soup.find_all("table", class_="display striped enhancedtable")

# Key = Team Name: Val = Relative url to player salary 
team_urls = {}
for row in table[0].find_all('tr'):
    all_hrefs = row.find_all('a')
    for href in all_hrefs:
        team_urls[href.text.strip()] = href['href'].replace('/compensation/', '')


# Key = Team Name: Val = Dataframe of player salaries
player_salaries_by_team = {}
for team_url in team_urls.values():
    # Find team abbreviation from url
    pattern = re.compile(r'team=[A-Z]{3}')
    team_abbreviation = pattern.findall(team_url)

    soup = get_soup(base_url + team_url)
    player_salaries_by_team[team_abbreviation[0].replace('team=', '')] = get_table_as_df(soup)
    

```


The function uses a dictionary to temporarily store the scraped data into and eventually converts it into a dataframe. Due to how the website is organized, the dictionary keys are the names of each MLB team. The value of each key in this dictionary is a dataframe containing salary information for each player on that team. Therefore, there is still some transformation that needs to happen before we can store this data into a database. 

Also notice how the re library is used. Since the url contains the team name abbreviation, we use regular expressions to find the three capitol letters (team name abbreviation) so that we can use those abbreviations as the keys in the ```player_salaries``` dictionary. 


# Step 2: Transform Salary Data
Now that we have a dictionary full of player salaries, we need to make some transformations to the data so that it can be stored in the relational database.


```python
for team in player_salaries_by_team.keys():
    player_salaries_by_team[team] = player_salaries_by_team[team][['player', 'pos', 'salary']]
    player_salaries_by_team[team]['salary'] = player_salaries_by_team[team]['salary'].apply(lambda x: x.replace('$', '').replace(',', ''))
    player_salaries_by_team[team]['player'] = player_salaries_by_team[team]['player'].apply(lambda x: x if re.match('^[\x00-\x7F]+$', x) else "ERROR")
    player_salaries_by_team[team]['team'] = team
``` 

This code iterates through each key of the dictionary and makes a couple of transformations
1. Irrelevant columns are filtered out of the dataframe
2. The salary column is applied a function that changes the formatting of the salary (removes dollar signs and commas) so that it can be stored correctly in the database which will be reading it as a currency, not a string. 
3. It checks if the player's name contains any Non-ASCII characters. These will cause problems when storing the data to the database. If there are any non-ascii characters, the name is changed to "ERROR". 
4. A ```team``` column is created, holding the abbreviation of the player's team name. Since we are iterating through the dictionary whose keys are the players' team abbreviation, we can use that iterator as the column's values. 


Lastly, the dataframes are still associated to each key in the dicitonary. The last step in the transformation stage is to merge all of the player data into one large dataframe using the ```concat``` function. 


```python
player_salaries = pd.concat(player_salaries_by_team.values(), ignore_index=True).rename(columns={"player": "player_name", "salary": "salary_usd"})
del(player_salaries_by_team)
```


# Step 3: Export Salary Data as CSV
We can export our ```player_salaries``` dataframe as a csv file using the ```to_csv``` method. This csv will be loaded into the database later. 

```python
player_salaries.to_csv('mlb_salary_data.csv', index=False)
```


# Step 4: Extract, Transform and Load Team Data
Along with the player salary data, I decided to include a small dataset which contains the actual MLB team names, their abbreviations, and a url to a website contianing their schdule. This can be found [here](https://gist.githubusercontent.com/purp/cb71552e6313a832ec3c/raw/48204f9a270e5e12e2105e7ab82859de8800b24c/schedule_urls.csv). We are only interested in the team name and abbreviations. A quick script is written to import this csv file as a dataframe and do minor transformations for ease of use in the future. 



```python
url = "https://gist.githubusercontent.com/purp/cb71552e6313a832ec3c/raw/48204f9a270e5e12e2105e7ab82859de8800b24c/schedule_urls.csv"
header= ['team_name', 'abbreviation', 'schedule']
team_abbreviations = pd.read_csv(url, names=header).drop('schedule', axis=1)
team_abbreviations['abbreviation'] = team_abbreviations['abbreviation'].apply(lambda x: x.upper())
team_abbreviations[['team_name', 'abbreviation']].to_csv('team_names.csv', index=False, columns=header[:2])
```

The resulting dataframe has two columns: the team name, and the team name's abbreviation. The schedule column in dropped. The abbreviations are three capitalized characters. 


# Step 5: Load Data into Postgre Database
The schema for both tables is created using SQL within the Postgre admin interface. Key points here include the data types. The position and team columns only require 3 chars, as they are both abbreviations. The player name column is text, and the salary column is of the MONEY data type. 

```sql
DROP TABLE IF EXISTS player_salary;
DROP TABLE IF EXISTS team_names;

CREATE TABLE player_salary(
	player_name TEXT,
	pos VARCHAR(3),
	salary_usd MONEY,
	team VARCHAR(3)
);

CREATE TABLE team_names(
	team_name VARCHAR(45),
	abbreviation VARCHAR(3) PRIMARY KEY       

);
```

A preview of the player salary data can be seen below.

<img src="https://i.imgur.com/OhZoFdc.jpg" width="500" alt="preview">
